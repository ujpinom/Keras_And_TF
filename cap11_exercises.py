# -*- coding: utf-8 -*-
"""Cap11_Exercises.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TXIEYssFHe7Ti5DrQSPIhjmXYU7eepMM
"""

import tensorflow as tf;import tensorflow.keras as keras
import numpy as np;

"""# Deep Learning"""

from functools import partial

Acciones_Comunes= partial(keras.layers.Dense,activation='elu',kernel_initializer='he_normal'
                          )

(x_train,y_train),(x_test,y_test)=keras.datasets.mnist.load_data()

x_train,x_test=x_train/255.,x_test/255.

indices04_train=y_train[(y_train==0)|(y_train==1)|(y_train==2)|(y_train==3)|(y_train==4)]
indices04_train.shape

x_train_A=x_train[(y_train==0)|(y_train==1)|(y_train==2)|(y_train==3)|(y_train==4)]
indices04_test=y_test[(y_test==0)|(y_test==1)|(y_test==2)|(y_test==3)|(y_test==4)]
x_test_B=x_test[(y_test==0)|(y_test==1)|(y_test==2)|(y_test==3)|(y_test==4)]

modelo1= keras.models.Sequential([keras.layers.Flatten(input_shape=[28,28]),Acciones_Comunes(100),Acciones_Comunes(100),
                                  Acciones_Comunes(100),Acciones_Comunes(100),Acciones_Comunes(100),
                                  Acciones_Comunes(5,activation='softmax',kernel_initializer='glorot_uniform')])

checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5",save_best_only=True)
early_stopping= keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)

modelo1.compile(optimizer=keras.optimizers.Adam(),loss='sparse_categorical_crossentropy',metrics=['accuracy'])

hist1= modelo1.fit(x_train_A,indices04_train,validation_split=0.08,epochs=100,callbacks=[checkpoint_cb,early_stopping])

modelo1.evaluate(x_test_B,indices04_test)

"""# Deep Learning on CIFAR10"""

keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()

X_train = X_train_full[5000:]
y_train = y_train_full[5000:]
X_valid = X_train_full[:5000]
y_valid = y_train_full[:5000]

modelo2=keras.models.Sequential()
modelo2.add(keras.layers.Flatten(input_shape=[32,32,3]))
for _ in range(20):
  modelo2.add(keras.layers.Dense(100,activation='elu',kernel_initializer='he_normal'))
modelo2.add(keras.layers.Dense(10,activation='softmax'))

import tensorboard;import os
early_stop= keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)
best_modelo=keras.callbacks.ModelCheckpoint("my_cifar10_model.h5", save_best_only=True)
run_index = 1 # increment every time you train the model
run_logdir = os.path.join(os.curdir, "my_cifar10_logs", "run_{:03d}".format(run_index))
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stop, best_modelo, tensorboard_cb]

modelo2.compile(optimizer=keras.optimizers.Nadam(learning_rate=5e-5,),loss='sparse_categorical_crossentropy',metrics=['accuracy'])

modelo2.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=100,callbacks=callbacks)

best_mo=keras.models.load_model("my_cifar10_model.h5")
best_mo.evaluate(X_test,y_test)

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir my_cifar10_logs

!pip install matplotlib

import math;import matplotlib.pyplot as plt
K = keras.backend

class ExponentialLearningRate(keras.callbacks.Callback):
  
    def __init__(self, factor):
        self.factor = factor
        self.rates = []
        self.losses = []
    def on_batch_end(self, batch, logs):
        self.rates.append(K.get_value(self.model.optimizer.lr))
        self.losses.append(logs["loss"])
        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)

def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):
    init_weights = model.get_weights()
    iterations = math.ceil(len(X) / batch_size) * epochs
    factor = np.exp(np.log(max_rate / min_rate) / iterations)
    init_lr = K.get_value(model.optimizer.lr)
    K.set_value(model.optimizer.lr, min_rate)
    exp_lr = ExponentialLearningRate(factor)
    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,
                        callbacks=[exp_lr])
    K.set_value(model.optimizer.lr, init_lr)
    model.set_weights(init_weights)
    return exp_lr.rates, exp_lr.losses

def plot_lr_vs_loss(rates, losses):
    plt.plot(rates, losses)
    plt.gca().set_xscale('log')
    plt.hlines(min(losses), min(rates), max(rates))
    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])
    plt.xlabel("Learning rate")
    plt.ylabel("Loss")

class OneCycleScheduler(keras.callbacks.Callback):
    def __init__(self, iterations, max_rate, start_rate=None,
                 last_iterations=None, last_rate=None):
        self.iterations = iterations
        self.max_rate = max_rate
        self.start_rate = start_rate or max_rate / 10
        self.last_iterations = last_iterations or iterations // 10 + 1
        self.half_iteration = (iterations - self.last_iterations) // 2
        self.last_rate = last_rate or self.start_rate / 1000
        self.iteration = 0
    def _interpolate(self, iter1, iter2, rate1, rate2):
        return ((rate2 - rate1) * (self.iteration - iter1)
                / (iter2 - iter1) + rate1)
    def on_batch_begin(self, batch, logs):
        if self.iteration < self.half_iteration:
            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)
        elif self.iteration < 2 * self.half_iteration:
            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,
                                     self.max_rate, self.start_rate)
        else:
            rate = self._interpolate(2 * self.half_iteration, self.iterations,
                                     self.start_rate, self.last_rate)
        self.iteration += 1
        K.set_value(self.model.optimizer.lr, rate)

keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

modelo= keras.models.Sequential()
modelo.add(keras.layers.Flatten(input_shape=[32,32,3]))

for _ in range(20):
  modelo.add(keras.layers.Dense(100,activation='selu',kernel_initializer='lecun_normal'))

modelo.add(keras.layers.AlphaDropout(rate=0.1))
modelo.add(keras.layers.Dense(10,activation='softmax'))


optimizer = keras.optimizers.SGD(lr=1e-3)
modelo.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

X_means = X_train.mean(axis=0)
X_stds = X_train.std(axis=0)
X_train_scaled = (X_train - X_means) / X_stds
X_valid_scaled = (X_valid - X_means) / X_stds
X_test_scaled = (X_test - X_means) / X_stds

batch_size = 128
rates, losses = find_learning_rate(modelo, X_train_scaled, y_train, epochs=1, batch_size=batch_size)
plot_lr_vs_loss(rates, losses)
plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])

modelo= keras.models.Sequential()
modelo.add(keras.layers.Flatten(input_shape=[32,32,3]))

for _ in range(20):
  modelo.add(keras.layers.Dense(100,activation='selu',kernel_initializer='lecun_normal'))

modelo.add(keras.layers.AlphaDropout(rate=0.1))
modelo.add(keras.layers.Dense(10,activation='softmax'))


optimizer = keras.optimizers.SGD(lr=1e-2)
modelo.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

n_epochs = 15
onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)
history = modelo.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[onecycle])

